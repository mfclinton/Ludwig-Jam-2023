{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "model_name = \"gpt2-xl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go back one levels to the root of the project\n",
    "os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model = transformers.GPT2LMHeadModel.from_pretrained(model_name).cuda()\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(model_name, use_fast=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the starting words\n",
    "words_generic_path = \"data/starting_words/words_generic.txt\"\n",
    "words_catlike_path = \"data/starting_words/words_catlike.txt\"\n",
    "words_creative_path = \"data/starting_words/words_creative.txt\"\n",
    "with open(words_generic_path, \"r\") as f:\n",
    "    words_generic = f.read().splitlines()\n",
    "\n",
    "with open(words_catlike_path, \"r\") as f:\n",
    "    words_catlike = f.read().splitlines()\n",
    "\n",
    "with open(words_creative_path, \"r\") as f:\n",
    "    words_creative = f.read().splitlines()\n",
    "\n",
    "# Combine all the words into one list and sample 3 words\n",
    "words = words_generic + words_catlike + words_creative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing banned tokens, i.e. any tokens that contain characters that are not alphanumeric, comma, space, period\n",
    "all_tokens = [tokenizer.decode(token_id) for token_id in list(range(50257))] \n",
    "# Regex to check for alphanumeric, comma, space, period\n",
    "regex_general = re.compile('^[a-zA-Z0-9, .]+$')\n",
    "# Keep the tokens that fail the regex\n",
    "banned_tokens_general_ids = []\n",
    "for token in all_tokens:\n",
    "    if not regex_general.match(token):\n",
    "        banned_tokens_general_ids.append(token)\n",
    "\n",
    "# Encode the tokens and convert to a tensor\n",
    "banned_tokens_general_ids = [tokenizer.encode(token)[0] for token in banned_tokens_general_ids]\n",
    "banned_tokens_general_ids = torch.tensor(banned_tokens_general_ids)\n",
    "\n",
    "# Same, but only alphanumeric and space\n",
    "regex_first = re.compile('^[a-zA-Z0-9 ]+$')\n",
    "banned_tokens_first_ids = []\n",
    "for token in all_tokens:\n",
    "    if not regex_first.match(token):\n",
    "        banned_tokens_first_ids.append(token)\n",
    "\n",
    "banned_tokens_first_ids = [tokenizer.encode(token)[0] for token in banned_tokens_first_ids]\n",
    "banned_tokens_first_ids = torch.tensor(banned_tokens_first_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_top_k_without_replacement(model, tokenizer, text, banned_tokens_ids, top_k=10, num_samples=3):\n",
    "    \"\"\" Given some text, sample num_samples next words, without replacement, \n",
    "    from the top k most likely words (weighted by their softmaxed logits).\n",
    "    \"\"\"\n",
    "    # Encode the text\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\").cuda()\n",
    "    # Use the model to get the logits for the last word\n",
    "    logits = model(input_ids).logits[0][-1]\n",
    "    # Convert to probabilities\n",
    "    logits = torch.softmax(logits, dim=0)\n",
    "    # Zero out any tokens that are not in the allowed tokens\n",
    "    logits[banned_tokens_ids] = 0\n",
    "    \n",
    "    # Get the top k words (probabilities and indices)\n",
    "    top_k_tokens = torch.topk(logits, top_k)\n",
    "    # Sample 3 words without replacement\n",
    "    sampled_indices = torch.multinomial(top_k_tokens.values, num_samples, replacement=False)\n",
    "    # Get the indices of the top k words\n",
    "    top_k_tokens = top_k_tokens.indices[sampled_indices]\n",
    "\n",
    "    # Decode the tokens\n",
    "    next_tokens = []\n",
    "    for token in top_k_tokens:\n",
    "        next_tokens.append(tokenizer.decode(token))\n",
    "    return next_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_entire_word_simple(model, tokenizer, context, current_word, banned_tokens_ids, top_k=10):\n",
    "    \"\"\" Simple case where assume have a starter token\n",
    "    Use this when we are generating a word from \"scratch\" not trying to continue a user's current word\n",
    "    \"\"\"\n",
    "    context = context + current_word\n",
    "    for _ in range(5):\n",
    "        # Generate a next token\n",
    "        next_token = sample_top_k_without_replacement(model, tokenizer, context, banned_tokens_ids, top_k=top_k, num_samples=1)[0]\n",
    "        # If the first character is a space, comma, or period, break\n",
    "        if next_token[0] in [\",\", \".\", \" \"]:\n",
    "            break\n",
    "        # Otherwise it is a continuation of the word and we wnat to keep it\n",
    "        else:\n",
    "            context += next_token\n",
    "            current_word += next_token\n",
    "    return current_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End to end game example:\n",
    "curr_tweet = \"\"\n",
    "\n",
    "# Pick the first word randomly from the starting lists\n",
    "current_choices = random.sample(words, 3)\n",
    "\n",
    "while True:\n",
    "    print(f\"Current tweet: {curr_tweet}\")\n",
    "    print(f\"Options:\\n 1. {current_choices[0]}\\n 2. {current_choices[1]}\\n 3. {current_choices[2]}\")\n",
    "\n",
    "    # Get the user's choice\n",
    "    user_choice = input()\n",
    "    # Check what the user chose\n",
    "    if user_choice == \"1\":\n",
    "        curr_tweet += current_choices[0]\n",
    "    elif user_choice == \"2\":\n",
    "        curr_tweet += current_choices[1]\n",
    "    elif user_choice == \"3\":\n",
    "        curr_tweet += current_choices[2]\n",
    "    else:\n",
    "        curr_tweet += user_choice\n",
    "\n",
    "    # Generate 3 possibilities to kick off the next word\n",
    "    first_next_tokens = sample_top_k_without_replacement(model, tokenizer, curr_tweet, banned_tokens_first_ids, top_k=20, num_samples=3)\n",
    "    # Then finish generating each word\n",
    "    next_words = []\n",
    "    for token in first_next_tokens:\n",
    "        next_words.append(generate_entire_word_simple(model, tokenizer, curr_tweet, token, banned_tokens_general_ids, top_k=20))\n",
    "    current_choices = next_words\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3e96315dbe1723ebca6ec75d6f193dc1353bb51f66fe97f146e1555476d4b2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
