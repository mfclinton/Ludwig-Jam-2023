{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\FastProgramming\\Unity\\Ludwig-Jam-2023\\PythonDev\\.venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "classifier = transformers.pipeline('zero-shot-classification', model='roberta-large-mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\n",
    "    \"halloween\",\n",
    "    \"olympics\",\n",
    "    \"SNOWING\",\n",
    "    \"caturday\",\n",
    "    \"tech\",\n",
    "    \"festival\",\n",
    "    \"dinnertime\",\n",
    "    \"gaming\",\n",
    "    \"cats\",\n",
    "    \"new release\",\n",
    "    \"wellness\",\n",
    "    \"travel\",\n",
    "    \"music\",\n",
    "    \"artifical intelligence\",\n",
    "    \"podcast\",\n",
    "    \"FRIDAY\",\n",
    "    \"controversy\",\n",
    "    \"greedy\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9611343741416931, 0.02133234404027462, 0.01753329671919346]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[168, 2, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_to_classify = \"Nothing beats waking up to a world covered in snow ‚ùÑÔ∏è‚ù§Ô∏è It's like nature's own magical wonderland! Time to bundle up and make some snow angels!\"\n",
    "trending_topics = ['SNOWING', 'Caturday', 'Tech']\n",
    "tweets_each = [3242, 2129, 544]\n",
    "current_followers = 319\n",
    "# 1 is the first time the tweet has been posted, 2 would be the second time, etc.\n",
    "timesteps_since_posted = 1\n",
    "\n",
    "def compute_followers(tweet_to_classify, trending_topics, tweets_each, current_followers, timesteps_since_posted):\n",
    "\n",
    "    class_probs = classifier(tweet_to_classify, trending_topics)[\"scores\"]\n",
    "    print(class_probs)\n",
    "\n",
    "    # Normalize the tweets per topic\n",
    "    relative_tweets_each = [x / sum(tweets_each) for x in tweets_each]\n",
    "\n",
    "    # Add the number of followers gained from each topic\n",
    "    per_topic_followers = []\n",
    "    for i in range(len(trending_topics)):\n",
    "        gained = class_probs[i] * relative_tweets_each[i] * current_followers\n",
    "        per_topic_followers.append(gained)\n",
    "\n",
    "    # Scale by days since posted (limit this to 3)\n",
    "    per_topic_followers = [int(x / timesteps_since_posted) for x in per_topic_followers]\n",
    "    return per_topic_followers\n",
    "\n",
    "compute_followers(tweet_to_classify, trending_topics, tweets_each, current_followers, timesteps_since_posted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5362964868545532, 0.3720773756504059, 0.09162615239620209]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[66, 27, 5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_to_classify = \"The recent controversy surrounding the festival industry is a reminder that greed can ruin even the most celebrated events. Let's hope that organizers prioritize safety and ethics over profits, and that we can all enjoy festivals that uplift and unite us! üéâ‚úåÔ∏è\"\n",
    "trending_topics = ['controvesy', 'greedy', 'festival']\n",
    "tweets_each = [5242, 3129, 2344]\n",
    "current_followers = 510\n",
    "timesteps_since_posted = 2\n",
    "\n",
    "compute_followers(tweet_to_classify, trending_topics, tweets_each, current_followers, timesteps_since_posted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3e96315dbe1723ebca6ec75d6f193dc1353bb51f66fe97f146e1555476d4b2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
